---
layout:     post
title:      "redis-2.redis的高并发与高可用"
subtitle:   " \"keep hungry keep foolish\""
date:       2019-05-25 12:00:00
author:     "Bz"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - redis
    
#     redis-2.redis的高并发与高可用：
    
---
## redis的高并发

```
存在瓶颈原因:单机只能支持几万的QPS
	解决方案:
		读写分离
			一主多从,master负责写,并复制数据同步到其他slave节点,slave节点负责读
			优点:水平扩容简单
```

		
### redis replication的核心机制

```
1.redis才用异步方式复制数据到slave节点,2.8版本开始,slave node会周期性地确认自己每次复制的数据量
2.一个master node可以配置多个slave node
3.slave node也可以连接其他的slave node
4.slave node做复制的时候,不会block master node的正常工作
5.slave node做复制的时候,不会block自己的查询操作,它会用旧的数据集来提供服务,
    但在复制完成的时候,需要删除旧数据集,加载新数据集,这个时候会停止对外服务
6.slave node主要用来做横向扩容,做读写分离,扩容的slave node可以提高读的吞吐量
```


### master持久化对主从架构的安全保障的意义

```
如果采用主从架构,那么必须开启master node的持久化
不能只用slave node作为master node的数据热备,因为如果关掉master的持久化,
可能在master宕机重启的时候数据是空的,经过复制后slave node数据也就丢失了
1.master节点必须要使用持久化机制 RDB、AOF
2.master的各种备份方案,万一本地文件丢了,可以从备份中选一份rdb去恢复master才能保证master有数据
    但即使才用了高可用机制,slave node可以自动接管master node,但是也可能sentinal还没检测到master failure,
    master node就自动重启了,还是可能导致上面的所有slave node数据集清空故障
```


### 主从架构的核心原理

```
当启动一个slave node的时候,它会发送一个PSYNC命令给master node
如果这是slave node重连master node,那么master node仅仅会复制给slave部分缺少的数据,
如果是slave node第一次连接master node,那么会出发一次full resynchronization
开始full resynchronization的时候,master会启动一个后台线程,开始生成一份RDB快照文件,同时还会将从客户端收到的所有命令缓存到内存中。
RBD文件生产完毕后,master会将这个RDB发送给slave,slave会先写入本地磁盘,然后从本地磁盘加载到内存中。
然后master会将内存中缓存的写命令发送给slave,slave也会同步这些数据
slave node如果跟master node有网络故障,断开了连接,会自动重连,master如果发现有多个slave node都来重新连接,
仅仅会启动一个rdb save 操作,用一份数据服务所有slave node主要用来做横向扩容,做读写分离,扩容的slave
```


### 主从复制的断点续传

```
从redis2.8开始,就支持主从复制的端点续传,如果主从复制过程中,网络连接断掉了,那么可以接着上次复制的地方,继续复制下去,而不是从头复制一份
master node会在内存中创建一个backlog,master和slave都会保存一个replica offset 还有一个master id
offset就是保存在backlog中的,如果master和slave网络连接断掉了,slave会让master从上次的replica offset开始继续复制
但如果没有找到对应的offset,那么就会执行一次resynchronization
```


### 无磁盘化复制

```
master 在内存中直接创建rdb,然后发送给slave,不会在本地落地磁盘
repl-diskless-sync
repl-diskless-sync-delay,等待一定时长再开始复制,因为要等更多slave重新连接过来
```


### 过期key处理

```
slave不会过期key,只会等待master过期key,如果master过期了一个key,或通过LRU淘汰了一个key,那么会模拟一条del命令发送给slave
```


### slave node复制master node的流程

```
1.slave node启动,仅保存master node的信息,包括master node的host、ip(配置文件读取)
2.slave node内部有个定时任务,每秒检查是否有新的master node要连接和复制,如果发现,就和master node建立socket连接
3.slave node发送ping命令给master node
4.口令认证,如果master设置了requirepass,那么salve node必须发送masterauth的口令过去进行认证
5.master node第一次执行全量复制,将所有数据发送给slave node
6.master node后续持续将写命令,同步复制给slave node
```


### 同步数据的核心机制

```
1.master和slave都会维护一个offset
	master会在自身不断累加offset,slave也会在自身不断累加offset
	slave每秒都会上报自己的offset给master,同时master也会保存每个slave的offset】
2.backlog
	master node有一个backlog,默认是1MB大小
	master node给slave node复制数据时,也会将数据在backlog中同步写一份
3.master run id
	info server,可以看到master run id
	如果根据host+ip定位master node是不靠谱的,因为如果master node重启或者数据出现了变化,那么slave node应该根据不同的run id区分,
	run id不同就做全量复制，如果需要不更爱run id重启redis,可以使用redis-cli debug reload命令
4.psync
	从节点使用psync从master node进行复制,psync runid offset
	master node会根据自身的情况返回响应信息,可能是FULLRESYNC runid offset触发全量复制,可能是CONTINUE触发增量复制
```

	
### 全量复制流程

```
1.master执行bgsave,在本地生成一份rdb快照文件
2.master node讲rdb快照文件发送给slave node,如果rdb复制时间超过60秒(repl-timeout),那么slave node就会认为复制失败
3.对于千兆网卡的机器,一般每秒传输100mb,6G文件很可能超过60秒
4.master node在生成rdb时,会将所有新的写命令缓存在内存中,在salve node保存了rdb之后,再将新的写命令复制给salve node
5.client-output-buffer-limit slave 256MB 64MB 60,如果在复制期间,内存缓冲区持续消耗超过64MB,或者一次性超过256MB,那么停止复制,复制失败
6.slave node接受rdb之后,清空自己的旧数据,然后重新加载rbd到自己的内存中,同时基于旧的数据版本对外提供服务
7.如果slave node开启了AOF,那么会立即执行BGREWRITEAOF,重写AOF
注:如果复制的数据量在4-6G,那么全量复制时间消耗在90-120秒间
```


### 增量复制流程

```
1.如果全量复制过程中,master-slave网络连接断掉,那么salve重新连接master时,会触发增量复制
2.master直接从自己的backlog中获取部分丢失的数据,发送给slave node,默认backlog就是1MB
3.master就是根据slave发送的psync中的offset来从backlog中获取数据的
```


### 心跳机制

```
主从节点互相都会发送heartbeat信息
master默认每隔10秒发送一次heartbeat,salve node每隔1秒发送一个heartbeat
```


### 异步复制

```
master每次接收到写命令之后,先在内部写入数据,然后异步发送给slave node
```


## redis的高可用

```
主从架构:
	原因:master宕机,写不可用,即缓存不可用
	解决方案:故障转移,failover,也叫主备切换,在master node故障时,自动检测,并将某个slave node自动切换为master node
		sentinal node:哨兵,监控master状态,一旦master故障,提升一个slave node为新的master
```


### 哨兵介绍

```
sentinal:哨兵
哨兵是redis集群架构中非常重要的一个组件,主要功能如下:
1.集群监控,负责监控redis master和slave进程是否正常工作
2.消息通知,如果某个redis实例有故障,那么哨兵负责发送消息作为报警通知给管理员
3.故障转移,如果master node挂掉了,会自动转移到slave node上
4.配置中心,如果故障转移发生了,通知client客户端新的master地址
哨兵本身也是分布式的,作为一个哨兵集群去运行,互相协同工作
1.故障仗义时,判断一个master node是宕机了,需要大部分的哨兵都同意才行,涉及到了分布式选举的问题
2.即使部分哨兵节点挂掉了,哨兵集群还是能正常工作的,因为如果一个作为高可用机制重要组成部分的故障转移系统本身肯定不能是单点的
目前采用的是sentinal 2版本,相对应版本1来说,重写了很多代码,主要让故障转移的机制和算法变得更加健壮和简单
```


### 哨兵的核心知识

```
1.至少要3个实例,来保证自己的健壮性
2.哨兵+redis主从的部署架构,是不会保证数据零丢失的,只能保证redis集群的高可用
3.对于哨兵+dedis主从这种复杂的不熟架构,尽量在测试环境和生产环境,都进行充足的测试和演练
```


### 为什么redis哨兵集群只有2个节点无法正常工作

```
master宕机,进行故障转移的时候需要大部分哨兵同意(majority)
2个哨兵的majority=2,3个哨兵的majority=2,4个哨兵的majority=2,5个哨兵的majority=3,当哨兵只剩1个,达不到这个条件,故障转移不会执行
```


### redis数据丢失

```
1.异步复制导致的数据丢失
	原因:master写操作还未来得及异步复制给slave的时候就挂掉了,所以有可能丢失部分数据

2.闹裂问题
	原因:master节点与哨兵连接异常,却与客户端连接正常,哨兵提升slave node为master,导致有2个master存活
	这时候写入的数据可能会因为master网络异常恢复以后降级为slave node导致丢失
解决方案:
	min-slaves-towrite 1
	min-slaves-max-lag 10
	要求至少有1个slave,数据复制和同步的延迟不能超过10秒
	一旦所有slave,数据复制和同步的延迟都超过了10秒钟,那么这个时候,master就不会再接受请求
	这个时候一般会在客户端再做处理,把未写入redis的数据持久化再处理、或写入消息队列,再定时尝试写入dedis
```

### redis哨兵的7个核心底层原理

```
1.sdown和odown转换机制
sdown和odown两种失败状态
sdown是主观宕机,就是一个哨兵如果自己觉得一个master宕机了,那么就是主观宕机
odown是客观宕机,如果quorum数量的哨兵都觉得一个master宕机了,那么就是客观宕机
sdown达成的条件:如果一个哨兵ping一个master,超过了is-master-down-after-milliseconds指定的毫秒数之后,就主观认为master宕机
sdown到odown转换的条件:如果一个哨兵在指定时间内,收到了quorum指定数量的其他哨兵也认为那个master是sdown了,那么就认为是odown
2.哨兵和slave集群的自动发现机制
哨兵互相之间发现,是通过redis的pub/sub系统实现的,每个哨兵会往4_sentinel_:hello这个channel里发送一个消息,
这个时候其他所有哨兵都可以消费到这个消息,并感知到其他哨兵的存在
每隔两秒钟,每个哨兵都会往自己监控的某个master+slaves对应的_sentinel_:hello channel里面发送一个消息,内容是自己的host、ip和runid
还有对这个master的监控配置
每个哨兵也会去监听自己监控的每个master+slave对应的_sentinel_:hello channel,然后去感知到同样在监听这个master+slaves的其他哨兵的存在
每个哨兵还会跟其他哨兵交换对master的监控配置,互相进行监控配置的同步
3.slave配置的自动纠正
哨兵会负责自动纠正slave的一些配置,比如slave如果要成为潜在的master候选人,哨兵会保证slave在复制现有master的数据
如果slave连接到了一个错误的master上,比如故障转移之后,那么哨兵会确保他们连接在正确的master是
4.slave-》master选举算法
如果一个master认为odown了,而且majority哨兵都允许了主备切换,那么某个哨兵就会执行主备切换操作,此时首先要选举出一个slave来
会考虑slave的一些信息
	1.与master断开连接的时长
	2.slave优先级
	3.复制offset
	4.run id
如果一个slave跟master断开连接已经超过了down-after-milliseconds的10倍,外加master宕机的时长,那么slave就被认为不适合选举为master
接下来会按照slave进行排序
	1.按照slave优先级进行排序,slave priority越低,优先级就越高
	2.如果slave priority相同,那么看replica offset 那个slave复制了越多的数据,offset越靠后,优先级就越高
	3.如果上面两个条件都相同,那么选择一个run id比较小的那个slave
5.quorum和majority
没一个哨兵要做主备切换,首先需要quorum数量的哨兵认为odown,然后选举出一个哨兵来做切换,这个哨兵还得得到majority哨兵的首选,才能正式执行切换
如果quorum<majority,比如5个哨兵,majority就是3,quorum设置为2,那么就是3个哨兵授权就可以执行切换
但如果quorum>=majority,那么必须quorum数量的哨兵都授权,比如5个哨兵,quorum是5,那么必须5个哨兵都同意授权,才能执行切换
6.configuration epoch
执行切换的那个哨兵,会从要切换到的新master哪里得到一个configuration epoch,这就是一个version号,每次切换的version都必须是唯一的
如果第一个选举出的哨兵切换失败了,那么其他哨兵,会等待failover-timeout时间,然后接替继续执行切换,
此时会重新获取一个新的configuration epoch,作为新的version号
7.configuration传播
哨兵完成切换之后,会在自己本地更新生成最新的master配置,然后同步给其他哨兵,就是通过前面说的pub/sub消息机制
这里之前的version号就很重要了,因为各种消息都是通过一个channel去发布和监听的,所以一个哨兵完成一次新的切换之后,
新的master配置是跟着新的version号的，其他的哨兵都是根据版本号的大小来更新自己的master配置的
```




























