---
layout:     post
title:      "Spark-2.sql"
subtitle:   " \"keep hungry keep foolish\""
date:       2019-05-25 12:00:00
author:     "Bz"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - Spark
    
#     Spark-Sql：Scala编写例子
---
## 相关Maven配置

```
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.12</artifactId>
            <version>2.4.3</version>
        </dependency>

        <!--oracle -->
        <dependency>
            <groupId>com.oracle</groupId>
            <artifactId>ojdbc6</artifactId>
            <version>11.2.0.3</version>
        </dependency>
        <!--mysql -->
        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>5.1.31</version>
        </dependency>
```

## sql方式处理数据

```
object SparkSql {
  def main(args: Array[String]): Unit = {
    //创建配置对象
    val conf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("SparkSql")
    val spark = new sql.SparkSession.Builder().config(conf).getOrCreate()
    //读取文件数据
    val frame: DataFrame = spark.read.json("in/1.json")
    //测试读取文件
    //    frame.show()
    //DataFrame 转化为视图
    frame.createTempView("user")
    //创建或替换
    //    frame.createOrReplaceTempView("user")
    //采用sql语法访问数据
    spark.sql("select * from user").show()
    //释放资源
    spark.stop()
  }
}
```
## 三种数据类型之间的转换

```
/**
  * SparkSql 三种类型转换 RDD、DataSet、DataFrame
  * RDD、DataSet 互相转换
  */
object SparkSqlTransform {
  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("SparkSqlTransform")
    val spark: SparkSession = new SparkSession.Builder().config(conf).getOrCreate()
    //加入隐式转换包
    import spark.implicits._
    //创建rdd
    val rdd: RDD[(Int, String, Int)] = spark.sparkContext.makeRDD(List((1, "bz", 16), (2, "jdb", 18)))
//    //rdd转df
//    val df: DataFrame = rdd.toDF("id", "name", "age")
//    //df转ds
//    val ds: Dataset[User] = df.as[User]
//    //ds转df
//    val df2: DataFrame = ds.toDF()
//    //df转rdd     ==>Row通过索引获取数据
//    val rdd2: RDD[Row] = df2.rdd
//    rdd2.foreach(row => {
//      println("id:" + row.getInt(0) + ",name:" + row.getString(1) + ",age:" + row.getInt(2))
//    })




    //rdd <==> ds
    val userRDD: RDD[User] = rdd.map {
      case (id, name, age) => {
        User(id, name, age)
      }
    }
    val userDs: Dataset[User] = userRDD.toDS()
    val rdd3: RDD[User] = userDs.rdd
    rdd3.foreach(println)

  }

}

case class User(id: Long, name: String, age: Long)
```

## 自定义聚合函数(强类型)

```

/**
  * 用户自定义强类型聚合函数
  */
object SparkSqlUdaf {
  def main(args: Array[String]): Unit = {

    val conf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("SparkSqlUdaf")
    val spark: SparkSession = new SparkSession.Builder().config(conf).getOrCreate()

    import spark.implicits._
    //创建实例
    val function = new MyAgeAvgClassFunction
    //聚合函数转换为查询列
    val column: TypedColumn[UserData, Double] = function.toColumn.name("avgAge")
    //调用自定义聚合函数
    val df: DataFrame = spark.read.json("in/1.json")
    val ds: Dataset[UserData] = df.as[UserData]

    val rs: Dataset[Double] = ds.select(column)
    rs.show()
//    println("平均年龄:"+rs)
    spark.stop()
  }
}

case class UserData(id: Long, name: String, age: Long)

case class AvgBuffer(var sum: Long, var count: Int)


class MyAgeAvgClassFunction extends Aggregator[UserData, AvgBuffer, Double] {
  //初始化
  override def zero: AvgBuffer = {
    AvgBuffer(0, 0)
  }

  //计算
  override def reduce(b: AvgBuffer, a: UserData): AvgBuffer = {
    b.sum = b.sum + a.age
    b.count = b.count + 1
    b
  }

  //合并
  override def merge(b1: AvgBuffer, b2: AvgBuffer): AvgBuffer = {
    b1.sum = b1.sum + b2.sum
    b1.count = b2.count
    b1
  }

  //输出
  override def finish(reduction: AvgBuffer): Double = {
    reduction.sum.toDouble / reduction.count
  }

  override def bufferEncoder: Encoder[AvgBuffer] = Encoders.product

  override def outputEncoder: Encoder[Double] = Encoders.scalaDouble
}
```
## SparkSql操作数据库

```
/**
  * spark对数据库操作
  */
object SparkDb {
  def main(args: Array[String]): Unit = {

    //创建配置对象
    val conf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("SparkSql")
    val spark = new sql.SparkSession.Builder().config(conf).getOrCreate()

    import spark.implicits._
    //    ================read========================
    //方式一
    //    val df1: DataFrame = spark.read.format("jdbc")
    //      .option("url", "jdbc:oracle:thin:@192.168.1.168:1521:orcl")
    //      .option("dbtable", "SYS_USER")
    //      .option("user", "d_wq_trade")
    //      .option("password", "d_wq_trade_123")
    //      .load()
    //    df1.show()
    //方式二
    //    val properties = new Properties()
    //    properties.put("user", "zxxoa")
    //    properties.put("password", "d_zxxoa_123")
    //    val df2: DataFrame = spark.read.jdbc("jdbc:mysql://192.168.1.246:3306/zxxoa", "SYS_USER", properties)
    //    df2.show()


    //    ================write========================

    //方式一
    //    val df: DataFrame = spark.sparkContext.makeRDD(List((1, "bz", 18))).toDF("id", "name", "age")
    //    df.write
    //      .format("jdbc")
    //      .option("url", "jdbc:oracle:thin:@192.168.1.168:1521:orcl")
    //      .option("dbtable", "SYS_USER")
    //      .option("user", "d_wq_trade")
    //      .option("password", "d_wq_trade_123")
    //      .save()
    //方式二
    //    df.write
    //      .jdbc("jdbc:oracle:thin:@192.168.1.168:1521:orcl", "SYS_USER", properties)


    spark.stop()
  }
}
```

## 项目地址
https://github.com/DuDuLuuu/Spark